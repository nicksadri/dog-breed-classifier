{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2387160,"sourceType":"datasetVersion","datasetId":453611}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport os\nimport cv2\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load in dataset\ntrain_dir = \"/kaggle/input/70-dog-breedsimage-data-set\"\ndata_df = pd.read_csv(\"/kaggle/input/70-dog-breedsimage-data-set/dogs.csv\")\ndata_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into training, validation, and test data\ntrain_df = data_df[data_df.iloc[:, 2] == \"train\"].copy()\nvalid_df = data_df[data_df.iloc[:, 2] == \"valid\"].copy()\ntest_df = data_df[data_df.iloc[:, 2] == \"test\"].copy()\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode labels\nencoded_train_labels, _ = pd.factorize(train_df[\"labels\"])\nencoded_valid_labels, _ = pd.factorize(valid_df[\"labels\"])\nencoded_test_labels, _ = pd.factorize(test_df[\"labels\"])\ntrain_df[\"encoded_labels\"] = encoded_train_labels\nvalid_df[\"encoded_labels\"] = encoded_valid_labels\ntest_df[\"encoded_labels\"] = encoded_test_labels\n\nprint(set(encoded_labels))\ndata_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unnecessary columns\ncolumns_to_drop = [\"labels\", \"data set\"]\ntrain_df = train_df.drop(columns = columns_to_drop)\nvalid_df = valid_df.drop(columns = columns_to_drop)\ntest_df = test_df.drop(columns = columns_to_drop)\nprint(train_df.head())\nprint(valid_df.head())\nprint(test_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make labels first column\nnew_order = [\"encoded_labels\", \"filepaths\"]\ntrain_df = train_df[new_order]\nvalid_df = valid_df[new_order]\ntest_df = test_df[new_order]\nprint(train_df.head())\nprint(valid_df.head())\nprint(test_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAINING\n# Convert filepaths to numpy arrays\ntrain_df[\"images\"] = train_dir + \"/\" + train_df[\"filepaths\"]\ntrain_df = train_df.drop(columns=[\"filepaths\"])\ntrain_df[\"images\"] = train_df[\"images\"].apply(lambda path: cv2.imread(path, cv2.IMREAD_GRAYSCALE))\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten images\npixels = np.stack(train_df['images'].values).reshape(len(train_df), -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create columns for individual pixels of images\npixels_df = pd.DataFrame(pixels, columns=[f\"pixel_{i}\" for i in range(pixels.shape[1])])\ntrain_df = pd.concat([train_df['encoded_labels'], pixels_df], axis=1)\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create numpy array representing all training images\ntrain_df = np.array(train_df)\nnp.random.shuffle(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate image data from labels\ntrain_arr = train_df\nX_train = train_arr[:, 1:] / 255\nY_train = train_arr[:, 0]\nm, n = X_train.shape\noutput_size = len(set(encoded_train_labels))\nhidden_size = n * 2 // 3 + output_size\nX_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define activation function for hidden layer\ndef relu(Z):\n    return np.maximum(0, Z)\n\ndef relu_derivative(Z):\n    return Z > 0\n\n# Define activation function for output layer\ndef softmax(Z):\n    Z -= np.max(Z, axis=0, keepdims=True)\n    A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n    return A","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialize weights and biases\ndef init_params():\n    W1 = np.random.randn(n, hidden_size)\n    b1 = np.zeros(hidden_size)\n    W2 = np.random.randn(hidden_size, output_size)\n    b2 = np.zeros(output_size)\n    \n    W1_gradient = np.zeros(n, hidden_size)\n    b1_gradient = np.zeros(hidden_size)\n    W2_gradient = np.zeros(hidden_size, output_size)\n    b2_gradient = np.zeros(output_size)\n    return W1, b1, W2, b2, W1_gradient, b1_gradient, W2_gradient, b2_gradient","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define forward propagation\ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = np.dot(X, W1) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(A1, W2) + b2\n    A2 = softmax(Z2)\n    loss = compute_cross_entropy_loss(A2, Y)\n    accuracy = compute_accuracy(A2, Y)\n    return Z1, A1, Z2, A2, loss, accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define backward propagation\ndef backward_prop(Z1, A1, Z2, A2, W1, b1, W2, b2, X, Y):\n    dLZ2 = S2.copy()\n    dLZ2[range(len(X)), y] -= 1\n    dLZ2 /= len(X)\n\n    dLW2 = np.dot(np.transpose(S1), dLZ2)\n    dB2 = np.sum(dLZ2, axis=0)\n\n    dLS1 = np.dot(dLZ2, np.transpose(W2)\n    dLZ1 = dLS1 * relu_derivative(Z1)\n\n    dLW1 = np.dot(np.transpose(X), dLZ1)\n    dB1 = np.sum(dLZ1, axis=0)\n\n    W1_gradient = dLW1\n    b1_gradient = dB1\n    W2_gradient = dLW2\n    b2_gradient = dB2\n    return W1_gradient, b1_gradient, W2_gradient, b2_gradient","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_accuracy(X_pred, Y):\n    preds = np.argmax(X_pred, axis=1)\n    accuracy = np.mean(preds == Y)\n    return accuracy\n\ndef compute_cross_entropy_loss(X_pred, Y):\n    log_likelihood = -np.log(X_pred[range(len(Y)), Y])\n    loss = np.sum(log_likelihood) / len(Y)\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}